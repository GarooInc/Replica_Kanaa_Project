# app/streaming/streaming.py

import asyncio
import logging
from typing import AsyncIterator, List, Dict, Any
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from utilities.photo_uploader import upload_first_photo_found
import json # for JSON encoding, on tool_log. 

from app.streaming.event_handler import send_event, send_error, send_done


# missing: import tools configuration and enhanced prompt. 

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def ask_streaming(
        question: str,
        message_history: List[Dict] = [],
        max_iterations: int = 5,
        tools: List[Any] = [],
        enhanced_prompt: str = "", # this should later be changed to a get function since preamble changes based on user question.
        base_llm: Any = None,
        tool_map: Dict[str, Any] = {}
    ) -> AsyncIterator[str]:

    """
    Streaming function using Langchain to handle conversational AI with tool usage.
    Streams tokens as they're generated by LLM. 
    """

    try: 
        logger.info(f"Starting streaming function for question: {question}")

        # validate empty questions. 
        if not question or not question.strip():
            async for chunk in send_event("answer", "Por favor, ingresa una pregunta."):
                yield chunk
            return
        
        # get prompt
        prompt = enhanced_prompt 

        # build message history
        history = [SystemMessage(content=prompt)]

        for msg in message_history:
            if msg.get("role") == "user":
                history.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") in ["agent", "assistant"]:
                history.append(AIMessage(content=msg.get("content", "")))

        history.append(HumanMessage(content=question))

        # Bind llm and tools. 
        llm = base_llm.bind_tools(tools)

        # initialize tool log
        tool_log: List[Dict[str, Any]] = [] # for logging tool usage. 

        # url_image: variable to hold uploaded image url if any.
        url_image = None # needs to be none, otherwise agent will detect it as having a url. 

        # ---------------------------------
        # Agent loop: ReAct pattern
        # ---------------------------------

        for iteration in range(max_iterations):
            logger.info(f"Iteration {iteration + 1} of {max_iterations}")
            try: 

                # Invoke LLM with current history
                res = llm.invoke(history)

                # check for tools. 
                tool_calls = getattr(res, "tool_calls", None)

                if not tool_calls:
                    # no tool calls, handle images if any.

                    # event tool - check tool log for any image content.
                    async for chunk in send_event("tool_usage", "checking for image content"):
                        yield chunk
                    try:
                        url_image = await upload_first_photo_found(res.content, url_image)  # might fail.
                    except Exception as e:
                        logger.error(f"Error uploading image: {e}")

                    async for chunk in send_event("tool_usage", f"Done checking for image content"):
                        yield chunk

                    # No tool calls, stream answer
                    async for chunk in llm.astream(history):  # real streaming using astream from llm
                        if hasattr(chunk, "content") and chunk.content:
                            # cada token se envía como evento SSE tipo "answer"
                            async for c in send_event("answer", {"content": chunk.content}):
                                yield c

                    # send url_image if any.
                    if url_image:
                        
                        # send url image in a markdown format.
                        url_image = url_image.strip() # 
                        url = f"![image]({url_image})"
                        async for chunk in send_event("image_url", {"url": url}):
                            yield chunk

                    # send tool log
                    if tool_log:
                        async for chunk in send_event("tool_log", tool_log):
                            yield chunk


                # tool calls. 
                else:
                    for tool_call in tool_calls:
                        tool_name = tool_call.get('name')
                        tool_args = tool_call.get('args', {})
                        tool_id = tool_call.get('id')

                        tool_log.append({
                            "iteration: ": iteration + 1,
                            "tool_name": tool_name,
                            "tool_args": tool_args
                        })

                        display_name = tool_name # later should be changed for a getter function.

                        # notify tool usage
                        async for chunk in send_event("tool_usage", f"Usando: {display_name}"):
                            yield chunk


                        if tool_name in tool_map:
                            try:
                                tool = tool_map[tool_name]

                                # execute tool
                                if hasattr(tool, "ainvoke"):
                                    tool_result = await tool.ainvoke(tool_args) # later we should check for ainvoke behavior. 
                                elif hasattr(tool, "run"):
                                    if len(tool_args) == 0:
                                        tool_result = tool.run("")
                                    elif len(tool_args) == 1:
                                        tool_result = tool.run(list(tool_args.values())[0])
                                    else:
                                        tool_result = tool.run(**tool_args)
                                else:
                                    tool_result = f"Tool {tool_name} no tiene método ejecutable." # might check for this later.

                                logger.info(f"Tool {tool_name} executed with result: {tool_result}")
                                
                                history.append(ToolMessage(
                                    content=str(tool_result),
                                    tool_call_id=tool_id
                                ))

                                async for chunk in send_event("tool_usage", f"Ha terminado: {display_name}"):
                                    yield chunk


                            except Exception as tool_error:
                                logger.error(f"Error executing tool {tool_name}: {tool_error}")
                                history.append(ToolMessage(
                                    content=f"Error ejecutando la herramienta {tool_name}: {tool_error}",
                                    tool_call_id=tool_id
                                ))

                                async for chunk in send_event("tool_usage", f"Error en: {display_name}"):
                                    yield chunk

                        else:
                            logger.error(f"Tool {tool_name} not found in tool_map.")
                            history.append(ToolMessage(
                                content=f"Herramienta {tool_name} no encontrada.",
                                tool_call_id=tool_id
                            ))

                            async for chunk in send_event("tool_usage", f"Herramienta no encontrada: {display_name}"):
                                yield chunk

            except Exception as iteration_error:
                logger.error(f"Error in iteration {iteration + 1}: {iteration_error}")
                async for chunk in send_error("answer", "Ha ocurrido un error durante la generación de la respuesta."):
                    yield chunk

        # after max iterations, send done.
        logger.warning(f"Max iterations {max_iterations} reached.")

        final_prompt = history + [HumanMessage(content="Por favor, proporciona una respuesta final concisa a la pregunta original.")]
        async for chunk in llm.astream(final_prompt):
            if hasattr(chunk, "content") and chunk.content:
                async for c in send_event("answer", {"content": chunk.content}):
                    yield c

        # send tool log
        if tool_log:
            async for chunk in send_event("tool_log", tool_log):
                yield chunk
        async for chunk in send_done():
            yield chunk


    except Exception as e:
        logger.error(f"Error in streaming function: {e}")
        
        async for chunk in send_error("answer", "Ha ocurrido un error inesperado. Por favor, intenta de nuevo más tarde."):
            yield chunk
        return
    
    
    
    